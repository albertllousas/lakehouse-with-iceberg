# Lake-house with apache Iceberg and AWS

## Description

This project explores and assess apache **Iceberg** as a backbone to implement a **lake-house approach** to store and access  
huge amounts of raw data produced in a some tech company.

As a business driving use-case the project will try to full-fill the following **usecase**:
- We need to store **6 millions of user clicks per day**, of course, it needs to scale, we are ambitious.
- We have two **access patterns** to this information:
  - **By day**, some ML apps will access to generate their predictions and detect patterns.
  - **By date range given an element-id**, BI would like to create some real-time BI dashboards around it, we are not expecting super performant reads, but we wouldn't like to have our analyst **waiting for minutes** to see the data.

As a non-functional requirements:
- As cheap as possible, less money the better.
- Scalable, we are ambitious, we will have trillions of data to store in the near future ;-p 
- Abstract, we would like to apply this solution to potentially store all our raw data generated by streaming platforms (such kafka), FE events, ETLs ...

### Project scope

In order to assess iceberg the project is divided in two parts: 

#### Writing side

365 Millions of user clicks are going to be inserted in S3 with Iceberg table format using a stand-alone application, here 
we will check how easy is to do so, how to deal with partitioning, AWS integrations and so on.

<img width="75%" src="docs/Iceberg_read_side.drawio.png"/>

The writing part is not covering different ingestion mechanisms such as getting streaming events, CDC tools or an ETL pipelines, 
we work with the assumption that there data is already there ready to be stored.

#### Reading side

The project is going to read the data stored in S3 in three different ways:

<img width="75%" src="docs/Iceberg-Page-2.drawio.png"/>

## Prerequisites

The project will mainly integrate with AWS services, infra-code for these integrations is not provided within the project,
hence you will need:

- Provide AWS [credentials](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)
- A glue catalog with a predefined database is also expected, just [create a DB](https://docs.aws.amazon.com/glue/latest/dg/start-data-catalog.html), nothing else.
- Redshift spectrum cluster linked to our glue catalog. Please follow these [steps.](https://docs.aws.amazon.com/redshift/latest/dg/querying-iceberg.html)

## Benchmark

TODO

## Trade-off analysis

TODO

## Next Steps

TODO